import streamlit as st
import os
from dotenv import load_dotenv
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import time
import shutil

# LangChain components
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever

# --- C·∫§U H√åNH & THI·∫æT L·∫¨P BAN ƒê·∫¶U ---
load_dotenv()
# (T√πy ch·ªânh ƒë∆∞·ªùng d·∫´n Tesseract n·∫øu c·∫ßn, v√≠ d·ª• cho Windows)
pytesseract.pytesseract.tesseract_cmd = r'E:\university\V.1\chuyendeCNTT\Tesseract\tesseract.exe'

VECTOR_STORES_MAIN_DIR = "vector_stores"
PDF_UPLOADS_DIR = "pdf_uploads"

os.makedirs(VECTOR_STORES_MAIN_DIR, exist_ok=True)
os.makedirs(PDF_UPLOADS_DIR, exist_ok=True)

# --- CSS T√ôY CH·ªàNH CHO GIAO DI·ªÜN MINIMALIST ---
st.set_page_config(page_title="Tr·ª£ l√Ω AI Ph√¢n t√≠ch", layout="wide")


def load_css():
    st.markdown("""
        <style>
            /* M√†u n·ªÅn ch√≠nh */
            .stApp {
                background-color: #1E1E1E;
            }

            /* Bo g√≥c v√† ƒë·ªï b√≥ng cho c√°c container */
            [data-testid="stSidebar"], [data-testid="stExpander"], .stChatMessage, [data-testid="stButton"] > button {
                border-radius: 10px;
            }

            [data-testid="stSidebar"] {
                 background-color: #252526;
                 border-right: 1px solid #444;
            }

            /* N√∫t b·∫•m */
            [data-testid="stButton"] > button {
                border: 1px solid #444;
            }

            /* Khu v·ª±c t·∫£i file */
            [data-testid="stFileUploader"] {
                border: 2px dashed #444;
                background-color: #252526;
                padding: 25px;
            }
            [data-testid="stFileUploader"] > label {
                font-weight: bold;
                color: #FAFAFA;
            }
        </style>
    """, unsafe_allow_html=True)


load_css()


# --- C√ÅC H√ÄM BACKEND ---

@st.cache_resource
def load_models():
    """T·∫£i v√† cache c√°c m√¥ h√¨nh AI."""
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    llm = GoogleGenerativeAI(model="gemini-2.0-flash-lite-001", google_api_key=os.getenv("GOOGLE_API_KEY"))
    return embeddings, llm


def extract_text_from_pdf_with_ocr(pdf_path):
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file PDF, t·ª± ƒë·ªông √°p d·ª•ng OCR cho c√°c trang scan."""
    with st.spinner(f"ƒêang tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ '{os.path.basename(pdf_path)}'..."):
        doc = fitz.open(pdf_path)
        full_text = ""
        for page_num, page in enumerate(doc):
            digital_text = page.get_text()
            if len(digital_text.strip()) < 100:
                st.sidebar.info(f"Trang {page_num + 1} ƒëang ƒë∆∞·ª£c OCR...")
                try:
                    pix = page.get_pixmap(dpi=300)
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                    ocr_text = pytesseract.image_to_string(img, lang='vie+eng')
                    full_text += ocr_text + "\n"
                except Exception as e:
                    st.sidebar.error(f"L·ªói OCR trang {page_num + 1}: {e}")
            else:
                full_text += digital_text + "\n"
    return full_text


def create_knowledge_base(pdf_path, embeddings, vector_store_path):
    """H·ªçc file PDF v√† l∆∞u v√†o c∆° s·ªü d·ªØ li·ªáu vector c·ªßa lu·ªìng chat hi·ªán t·∫°i."""
    document_text = extract_text_from_pdf_with_ocr(pdf_path)
    if not document_text.strip():
        st.error("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n n√†o t·ª´ file PDF.")
        return

    with st.spinner("ƒêang x√¢y d·ª±ng c∆° s·ªü tri th·ª©c..."):
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
        chunks = text_splitter.create_documents([document_text], metadatas=[{"source": pdf_path}])
        db = Chroma.from_documents(chunks, embeddings, persist_directory=vector_store_path)
        db.persist()
    st.success(f"ƒê√£ h·ªçc th√†nh c√¥ng file: {os.path.basename(pdf_path)}")


def create_advanced_qa_chain(embeddings, llm, vector_store_path):
    """T·∫°o chu·ªói Q&A t·ª´ c∆° s·ªü d·ªØ li·ªáu vector c·ªßa lu·ªìng chat hi·ªán t·∫°i."""
    if not os.path.exists(vector_store_path) or not any(os.scandir(vector_store_path)):
        return None
    vectorstore = Chroma(persist_directory=vector_store_path, embedding_function=embeddings)
    retriever = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(search_kwargs={"k": 5}), llm=llm)
    prompt_template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia, c√≥ kh·∫£ nƒÉng ph√¢n t√≠ch v√† suy lu·∫≠n s√¢u s·∫Øc.
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng b·∫±ng c√°ch T·ªîNG H·ª¢P, SUY LU·∫¨N v√† R√öT RA C√ÅC K·∫æT LU·∫¨N LOGIC t·ª´ nh·ªØng th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn 'NG·ªÆ C·∫¢NH'.
    H∆Ø·ªöNG D·∫™N SUY LU·∫¨N:
    - ƒê·ª´ng ch·ªâ tr√≠ch xu·∫•t th√¥ng tin. H√£y h√†nh ƒë·ªông nh∆∞ m·ªôt chuy√™n gia, k·∫øt n·ªëi c√°c √Ω t∆∞·ªüng, ch·ªâ ra c√°c h√†m √Ω v√† cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi to√†n di·ªán, ngay c·∫£ khi c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c vi·∫øt nguy√™n vƒÉn trong t√†i li·ªáu.
    - ƒê∆∞a ra c√°c ph√¢n t√≠ch "d·ª±a tr√™n" v√† "suy ra t·ª´" c√°c d·ªØ ki·ªán trong ng·ªØ c·∫£nh.
    - Tuy nhi√™n, m·ªçi suy lu·∫≠n c·ªßa b·∫°n ph·∫£i ƒë∆∞·ª£c b√°m r·ªÖ v·ªØng ch·∫Øc v√†o c√°c d·ªØ ki·ªán c√≥ trong ng·ªØ c·∫£nh. Kh√¥ng ƒë∆∞·ª£c b·ªãa ƒë·∫∑t th√¥ng tin.
    - N·∫øu ng·ªØ c·∫£nh ho√†n to√†n kh√¥ng li√™n quan ho·∫∑c kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.
    - Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch chuy√™n nghi·ªáp v√† s√¢u s·∫Øc.
    - V·ªÅ b·ªë c·ª•c c·ªßa c√¢u tr·∫£ l·ªùi, n·∫øu n√≥ d√†i v√† c√≥ nhi·ªÅu √Ω, h√£y tr√¨nh b√†y t√°ch ƒëo·∫°n v√† xu·ªëng d√≤ng m·ªôt c√°ch khoa h·ªçc.
    NG·ªÆ C·∫¢NH: {context}
    C√ÇU H·ªéI: {question}
    C√ÇU TR·∫¢ L·ªúI C·ª¶A CHUY√äN GIA (ƒë√£ ph√¢n t√≠ch v√† suy lu·∫≠n):"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever,
                                           return_source_documents=True, chain_type_kwargs={"prompt": PROMPT})
    return qa_chain


# --- TI√äU ƒê·ªÄ & KH·ªûI T·∫†O ---
st.title("üöÄ Tr·ª£ l√Ω AI Ph√¢n t√≠ch T√†i li·ªáu")

if 'chats' not in st.session_state:
    st.session_state.chats = {}
if 'active_chat_id' not in st.session_state:
    st.session_state.active_chat_id = None
if 'confirm_delete' not in st.session_state:
    st.session_state.confirm_delete = None

if not st.session_state.chats:
    first_chat_id = f"D·ª± √°n {int(time.time())}"
    st.session_state.chats[first_chat_id] = {"title": "D·ª± √°n m·ªõi", "messages": [],
                                             "vector_store_path": os.path.join(VECTOR_STORES_MAIN_DIR, first_chat_id),
                                             "learned_files": set(), "latest_sources": "Ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi n√†o."}
    st.session_state.active_chat_id = first_chat_id

embeddings, llm = load_models()

# --- SIDEBAR: ƒêI·ªÄU KHI·ªÇN & QU·∫¢N L√ù D·ª∞ √ÅN ---
with st.sidebar:
    st.header("‚öôÔ∏è B·∫£ng ƒëi·ªÅu khi·ªÉn")
    if st.button("‚ûï D·ª± √°n m·ªõi", use_container_width=True):
        new_chat_id = f"D·ª± √°n {int(time.time())}"
        st.session_state.chats[new_chat_id] = {"title": "D·ª± √°n m·ªõi kh√¥ng t√™n", "messages": [],
                                               "vector_store_path": os.path.join(VECTOR_STORES_MAIN_DIR, new_chat_id),
                                               "learned_files": set(), "latest_sources": "Ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi n√†o."}
        st.session_state.active_chat_id = new_chat_id
        st.rerun()

    st.subheader("üìÅ C√°c d·ª± √°n")
    chat_ids = sorted(st.session_state.chats.keys(), reverse=True)
    for chat_id in chat_ids:
        col1, col2 = st.columns([4, 1])
        chat_title = st.session_state.chats[chat_id]['title']
        button_type = "primary" if chat_id == st.session_state.active_chat_id else "secondary"
        with col1:
            if st.button(chat_title, key=f"switch_{chat_id}", use_container_width=True, type=button_type):
                st.session_state.active_chat_id = chat_id
                st.session_state.confirm_delete = None  # H·ªßy x√≥a khi chuy·ªÉn lu·ªìng
                st.rerun()
        with col2:
            if st.button("üóëÔ∏è", key=f"delete_{chat_id}", use_container_width=True):
                st.session_state.confirm_delete = chat_id
                st.rerun()

    if st.session_state.confirm_delete:
        chat_to_delete_id = st.session_state.confirm_delete
        chat_to_delete_title = st.session_state.chats[chat_to_delete_id]['title']
        st.warning(f"B·∫°n c√≥ ch·∫Øc mu·ªën x√≥a vƒ©nh vi·ªÖn d·ª± √°n '{chat_to_delete_title}' kh√¥ng?")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("‚úÖ X√°c nh·∫≠n", use_container_width=True, type="primary"):
                vector_path_to_delete = st.session_state.chats[chat_to_delete_id]['vector_store_path']
                del st.session_state.chats[chat_to_delete_id]
                if os.path.exists(vector_path_to_delete):
                    shutil.rmtree(vector_path_to_delete)

                st.session_state.confirm_delete = None
                if st.session_state.active_chat_id == chat_to_delete_id:
                    st.session_state.active_chat_id = next(iter(st.session_state.chats), None)
                st.rerun()
        with col2:
            if st.button("‚ùå H·ªßy", use_container_width=True):
                st.session_state.confirm_delete = None
                st.rerun()

    st.divider()

    if st.session_state.active_chat_id:
        active_chat = st.session_state.chats[st.session_state.active_chat_id]
        st.subheader(f"Qu·∫£n l√Ω t√†i li·ªáu")
        uploaded_file = st.file_uploader("K√©o & th·∫£ file PDF v√†o ƒë√¢y", type="pdf", label_visibility="collapsed")
        if uploaded_file:
            file_path = os.path.join(PDF_UPLOADS_DIR, uploaded_file.name)
            if not os.path.exists(file_path):
                with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
            if st.button("H·ªçc File N√†y", use_container_width=True):
                create_knowledge_base(file_path, embeddings, active_chat["vector_store_path"])
                active_chat["learned_files"].add(uploaded_file.name)

        st.write("C√°c file ƒë√£ h·ªçc:")
        if active_chat["learned_files"]:
            for file_name in active_chat["learned_files"]: st.markdown(f"- `{file_name}`")
        else:
            st.write("Ch∆∞a c√≥ file n√†o.")

    st.divider()

    if st.session_state.active_chat_id:
        st.subheader("üìö Ngu·ªìn tr√≠ch d·∫´n")
        st.container(height=300).markdown(st.session_state.chats[st.session_state.active_chat_id]["latest_sources"],
                                          unsafe_allow_html=True)

# --- KHU V·ª∞C CH√çNH: GIAO DI·ªÜN CHAT ---
active_chat_id = st.session_state.active_chat_id
if active_chat_id:
    active_chat = st.session_state.chats[active_chat_id]
    new_title = st.text_input("T√™n d·ª± √°n:", value=active_chat["title"], label_visibility="collapsed")
    if new_title != active_chat["title"]:
        st.session_state.chats[active_chat_id]["title"] = new_title
        st.rerun()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in active_chat["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    qa_chain = create_advanced_qa_chain(embeddings, llm, active_chat["vector_store_path"])

    if user_question := st.chat_input("ƒê·∫∑t c√¢u h·ªèi v·ªÅ c√°c t√†i li·ªáu trong d·ª± √°n n√†y..."):
        if not qa_chain:
            st.warning("D·ª± √°n n√†y ch∆∞a c√≥ ki·∫øn th·ª©c. Vui l√≤ng 'H·ªçc' m·ªôt file PDF ·ªü thanh b√™n.")
            st.stop()

        # L·∫•y ti√™u ƒë·ªÅ cho chat n·∫øu ƒë√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n
        if not active_chat["messages"]:
            st.session_state.chats[active_chat_id]["title"] = user_question

        active_chat["messages"].append({"role": "user", "content": user_question})

        with st.spinner("AI ƒëang ph√¢n t√≠ch v√† suy lu·∫≠n..."):
            response = qa_chain.invoke(user_question)
            answer = response['result']

            sources_text = ""
            if 'source_documents' in response and response['source_documents']:
                for doc in response['source_documents']:
                    source_name = os.path.basename(doc.metadata.get('source', 'Kh√¥ng r√µ'))
                    content = doc.page_content.replace('\n', ' ').strip()
                    sources_text += f"<div style='border: 1px solid #444; border-radius: 5px; padding: 10px; margin-bottom: 10px;'><b>Ngu·ªìn:</b> <code>{source_name}</code><br><small><b>N·ªôi dung:</b> ...{content[:250]}...</small></div>"
            else:
                sources_text = "Kh√¥ng t√¨m th·∫•y ngu·ªìn tr√≠ch d·∫´n n√†o."

            active_chat["latest_sources"] = sources_text
            active_chat["messages"].append({"role": "assistant", "content": answer})
            st.rerun()

else:
    st.info("Ch√†o m·ª´ng b·∫°n! H√£y t·∫°o m·ªôt 'D·ª± √°n m·ªõi' ·ªü thanh b√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu.")